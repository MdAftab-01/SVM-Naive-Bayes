{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**SVM & Naive bayes**"
      ],
      "metadata": {
        "id": "fpJt9VPWLfFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **1. What is a Support Vector Machine (SVM)?**\n",
        "\n",
        "**SVM** is a supervised machine learning algorithm used for **classification** and **regression**.\n",
        "Its goal is to find the **best hyperplane** that separates different classes with the **maximum margin**.\n",
        "\n",
        "üìå Example: Separating spam vs non-spam emails.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. What is the difference between Hard Margin and Soft Margin SVM?**\n",
        "\n",
        "| Feature     | **Hard Margin**                           | **Soft Margin**                      |\n",
        "| ----------- | ----------------------------------------- | ------------------------------------ |\n",
        "| Definition  | No misclassifications allowed             | Allows some misclassifications       |\n",
        "| Use Case    | Only when data is **perfectly separable** | When data has **overlap or noise**   |\n",
        "| Flexibility | Rigid                                     | More flexible (uses **C parameter**) |\n",
        "\n",
        "---\n",
        "\n",
        "### **3. What is the mathematical intuition behind SVM?**\n",
        "\n",
        "SVM tries to **maximize the margin** between two classes by finding a hyperplane (a line in 2D, a plane in 3D, etc.).\n",
        "\n",
        "üî∏ The larger the margin, the better the generalization.\n",
        "\n",
        "Mathematically, it solves:\n",
        "\n",
        "$$\n",
        "\\min \\|w\\|^2 \\quad \\text{subject to } y_i(w \\cdot x_i + b) \\geq 1\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $w$ is the weight vector\n",
        "* $b$ is the bias\n",
        "* $y_i$ are the class labels\n",
        "* $x_i$ are the input features\n",
        "\n",
        "---\n",
        "\n",
        "### **4. What is the role of Lagrange Multipliers in SVM?**\n",
        "\n",
        "They help convert the **constrained optimization** problem of SVM into a **dual problem**, which is easier to solve using optimization techniques.\n",
        "\n",
        "* Only points **on or inside the margin** get **non-zero Lagrange multipliers**\n",
        "* These points are the **support vectors**\n",
        "\n",
        "---\n",
        "\n",
        "### **5. What are Support Vectors in SVM?**\n",
        "\n",
        "**Support Vectors** are the **data points closest to the decision boundary (hyperplane)**.\n",
        "\n",
        "üìå These points:\n",
        "\n",
        "* Directly affect the position of the hyperplane\n",
        "* Are **critical** to the final model\n",
        "\n",
        "---\n",
        "\n",
        "### **6. What is a Support Vector Classifier (SVC)?**\n",
        "\n",
        "`SVC` is the classification version of SVM in **Scikit-learn**.\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(kernel='linear')\n",
        "```\n",
        "\n",
        "It uses SVM to **classify** data into categories.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. What is a Support Vector Regressor (SVR)?**\n",
        "\n",
        "`SVR` is the **regression version** of SVM.\n",
        "Instead of separating classes, it tries to fit a line or curve **within a certain margin of error (epsilon tube)**.\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVR\n",
        "model = SVR(kernel='rbf')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **8. What is the Kernel Trick in SVM?**\n",
        "\n",
        "The **Kernel Trick** allows SVM to work in **non-linear spaces** without transforming the actual data.\n",
        "\n",
        "üìå Instead of transforming data into high-dimensional space, we compute the **dot product** using a **kernel function**.\n",
        "\n",
        "‚úÖ Examples of kernels:\n",
        "\n",
        "* Linear\n",
        "* Polynomial\n",
        "* RBF (Radial Basis Function)\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel**\n",
        "\n",
        "| **Kernel**     | **Use Case**                    | **Equation**                          | **Strength**               |\n",
        "| -------------- | ------------------------------- | ------------------------------------- | -------------------------- |\n",
        "| Linear         | When data is linearly separable | $K(x, y) = x \\cdot y$                 | Simple, fast               |\n",
        "| Polynomial     | For curved decision boundaries  | $K(x, y) = (x \\cdot y + c)^d$         | Can model complex patterns |\n",
        "| RBF (Gaussian) | When data is highly non-linear  | $K(x, y) = \\exp(-\\gamma \\|x - y\\|^2)$ | Powerful and flexible      |\n",
        "\n",
        "---\n",
        "\n",
        "### **10. What is the effect of the C parameter in SVM?**\n",
        "\n",
        "The **C parameter** controls the **trade-off between margin size and classification error**.\n",
        "\n",
        "* **Low C** ‚Üí Larger margin, more tolerance for misclassification (good generalization)\n",
        "* **High C** ‚Üí Smaller margin, less tolerance (tries to classify every point correctly)\n",
        "\n",
        "üìå In Scikit-learn:\n",
        "\n",
        "```python\n",
        "SVC(C=1.0, kernel='linear')\n",
        "```\n",
        "\n",
        "### **11. What is the role of the Gamma parameter in RBF Kernel SVM?**\n",
        "\n",
        "**Gamma (Œ≥)** controls **how far the influence of a single training example reaches** in the **RBF (Radial Basis Function) kernel**.\n",
        "\n",
        "* **High gamma** ‚Üí Each point has **short-range influence**, leading to a **complex, overfitted model**\n",
        "* **Low gamma** ‚Üí Each point influences a **wider area**, leading to a **smoother, simpler model**\n",
        "\n",
        "üìå Example in Scikit-learn:\n",
        "\n",
        "```python\n",
        "SVC(kernel='rbf', gamma=0.1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **12. What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?**\n",
        "\n",
        "**Na√Øve Bayes** is a probabilistic classifier based on **Bayes‚Äô Theorem**.\n",
        "\n",
        "It's called \"**Na√Øve**\" because it **assumes all features are independent**, which is rarely true in real-world data ‚Äî but still works well in practice.\n",
        "\n",
        "---\n",
        "\n",
        "### **13. What is Bayes‚Äô Theorem?**\n",
        "\n",
        "Bayes' Theorem allows us to update probabilities based on new evidence:\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $P(A|B)$: Posterior probability (after seeing evidence)\n",
        "* $P(B|A)$: Likelihood\n",
        "* $P(A)$: Prior probability\n",
        "* $P(B)$: Evidence (normalizing factor)\n",
        "\n",
        "---\n",
        "\n",
        "### **14. Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes**\n",
        "\n",
        "| Variant            | Use Case                                         | Assumes...                                           |\n",
        "| ------------------ | ------------------------------------------------ | ---------------------------------------------------- |\n",
        "| **Gaussian NB**    | Continuous data (e.g., age, height)              | Features follow a **normal (Gaussian)** distribution |\n",
        "| **Multinomial NB** | Count-based features (e.g., word counts in text) | Features are **frequencies/counts**                  |\n",
        "| **Bernoulli NB**   | Binary features (e.g., presence/absence of word) | Features are **binary (0 or 1)**                     |\n",
        "\n",
        "---\n",
        "\n",
        "### **15. When should you use Gaussian Na√Øve Bayes over other variants?**\n",
        "\n",
        "Use **Gaussian Na√Øve Bayes** when your features are **continuous and normally distributed**.\n",
        "\n",
        "üìå Example: Predicting if a person has a disease based on age, weight, and temperature.\n",
        "\n",
        "---\n",
        "\n",
        "### **16. What are the key assumptions made by Na√Øve Bayes?**\n",
        "\n",
        "1. **Feature independence** ‚Äì All features are independent of each other\n",
        "2. **Equal importance** ‚Äì Every feature contributes equally\n",
        "3. **Class-conditional independence** ‚Äì Features are independent **given the class**\n",
        "\n",
        "‚ö† These assumptions are usually false, but the algorithm still works well.\n",
        "\n",
        "---\n",
        "\n",
        "### **17. What are the advantages and disadvantages of Na√Øve Bayes?**\n",
        "\n",
        "‚úÖ **Advantages:**\n",
        "\n",
        "* Very **fast** and **simple**\n",
        "* Works well with **high-dimensional** data\n",
        "* Requires **less training data**\n",
        "* Great for **text classification**\n",
        "\n",
        "‚ùå **Disadvantages:**\n",
        "\n",
        "* Assumes **independence** (not realistic)\n",
        "* Not suitable when features are **highly correlated**\n",
        "\n",
        "---\n",
        "\n",
        "### **18. Why is Na√Øve Bayes a good choice for text classification?**\n",
        "\n",
        "* Text data is **high-dimensional** but sparse (many 0s), which Na√Øve Bayes handles well\n",
        "* Word counts/frequencies fit **Multinomial Na√Øve Bayes** perfectly\n",
        "* It‚Äôs **fast**, even for large datasets like spam detection, sentiment analysis, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### **19. Compare SVM and Na√Øve Bayes for classification tasks**\n",
        "\n",
        "| Feature          | **SVM**               | **Na√Øve Bayes**                       |\n",
        "| ---------------- | --------------------- | ------------------------------------- |\n",
        "| Type             | Discriminative        | Generative                            |\n",
        "| Speed            | Slower to train       | Very fast                             |\n",
        "| Assumptions      | No strict assumptions | Assumes feature independence          |\n",
        "| Small datasets   | Performs very well    | Performs well if independence holds   |\n",
        "| Text data        | Good, but slower      | Excellent (especially Multinomial NB) |\n",
        "| Interpretability | Lower                 | Higher                                |\n",
        "\n",
        "---\n",
        "\n",
        "### **20. How does Laplace Smoothing help in Na√Øve Bayes?**\n",
        "\n",
        "**Laplace Smoothing** fixes the problem of **zero probability** when a word/category **doesn‚Äôt appear** in the training data.\n",
        "\n",
        "üî∏ It adds a small constant (usually 1) to **every count**:\n",
        "\n",
        "$$\n",
        "P(word|class) = \\frac{\\text{count(word in class)} + 1}{\\text{total words in class} + V}\n",
        "$$\n",
        "\n",
        "Where $V$ = total number of unique words (vocabulary size)\n",
        "\n",
        "üìå Prevents multiplying by zero and improves generalization.\n"
      ],
      "metadata": {
        "id": "8bYJtXIcLmvG"
      }
    }
  ]
}